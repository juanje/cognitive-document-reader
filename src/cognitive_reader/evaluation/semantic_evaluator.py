"""Main semantic evaluator that runs all tests and generates reports."""

from __future__ import annotations

import json
from dataclasses import dataclass
from pathlib import Path
from typing import Any

from rich.console import Console
from rich.table import Table

from .base_test import TestOutcome, TestResult
from .test_definitions import (
    CausalRelationshipTest,
    ConceptualIntegrityTest,
    MetaphorFidelityTest,
    NuanceAmbiguityTest,
)


@dataclass
class EvaluationReport:
    """Complete evaluation report for a document."""

    document_title: str
    language: str
    total_tests: int
    passed_tests: int
    partial_tests: int
    failed_tests: int
    unable_to_evaluate: int
    overall_score: float
    test_outcomes: list[TestOutcome]

    @property
    def pass_rate(self) -> float:
        """Calculate pass rate (pass + partial) / total."""
        if self.total_tests == 0:
            return 0.0
        return (self.passed_tests + self.partial_tests) / self.total_tests

    @property
    def quality_grade(self) -> str:
        """Get quality grade based on overall score."""
        if self.overall_score >= 0.85:
            return "A+ Excellent"
        elif self.overall_score >= 0.75:
            return "A Good"
        elif self.overall_score >= 0.65:
            return "B+ Acceptable"
        elif self.overall_score >= 0.50:
            return "B Needs improvement"
        elif self.overall_score >= 0.35:
            return "C Poor"
        else:
            return "F Failed"


class SemanticEvaluator:
    """Main semantic evaluator for cognitive reading quality assessment."""

    def __init__(self) -> None:
        """Initialize evaluator with all available tests."""
        self.tests = [
            ConceptualIntegrityTest(),
            MetaphorFidelityTest(),
            CausalRelationshipTest(),
            NuanceAmbiguityTest(),
        ]
        self.console = Console()

    def load_knowledge_data(self, json_path: Path) -> dict[str, Any]:
        """Load processed knowledge data from JSON file.

        Args:
            json_path: Path to the JSON file generated by cognitive reader

        Returns:
            Knowledge data dictionary

        Raises:
            FileNotFoundError: If JSON file doesn't exist
            json.JSONDecodeError: If JSON is invalid
        """
        if not json_path.exists():
            raise FileNotFoundError(f"Knowledge JSON file not found: {json_path}")

        try:
            with open(json_path, encoding="utf-8") as f:
                data: dict[str, Any] = json.load(f)
                return data
        except json.JSONDecodeError as e:
            raise json.JSONDecodeError(
                f"Invalid JSON in {json_path}: {e}", "", 0
            ) from e

    def evaluate_document(
        self, json_path: Path, verbose: bool = False
    ) -> EvaluationReport:
        """Evaluate a processed document against all semantic tests.

        Args:
            json_path: Path to the JSON file generated by cognitive reader
            verbose: Whether to print detailed progress

        Returns:
            Complete evaluation report
        """
        if verbose:
            self.console.print(f"ðŸ§  Loading knowledge data from: {json_path}")

        knowledge_data = self.load_knowledge_data(json_path)

        document_title = knowledge_data.get("document_title", "Unknown Document")
        language = knowledge_data.get("detected_language", "unknown")

        if verbose:
            self.console.print(f"ðŸ“š Evaluating: {document_title} ({language})")
            self.console.print(f"ðŸ§ª Running {len(self.tests)} semantic tests...")

        test_outcomes = []

        for test in self.tests:
            if verbose:
                self.console.print(f"  â³ Running: {test.name}")

            try:
                outcome = test.evaluate(knowledge_data)
                test_outcomes.append(outcome)

                if verbose:
                    status_icon = self._get_status_icon(outcome.result)
                    self.console.print(
                        f"    {status_icon} {outcome.result.value} (Score: {outcome.score:.2f})"
                    )

            except Exception as e:
                # Handle test failures gracefully
                error_outcome = TestOutcome(
                    test_name=test.name,
                    result=TestResult.UNABLE_TO_EVALUATE,
                    score=0.0,
                    details=f"Test failed with error: {str(e)}",
                    expected_answer="Test could not be completed",
                    found_evidence="Error occurred during evaluation",
                    failure_reason=f"Exception: {str(e)}",
                )
                test_outcomes.append(error_outcome)

                if verbose:
                    self.console.print(f"    âŒ ERROR: {str(e)}")

        # Calculate summary statistics
        passed = sum(
            1 for outcome in test_outcomes if outcome.result == TestResult.PASS
        )
        partial = sum(
            1 for outcome in test_outcomes if outcome.result == TestResult.PARTIAL
        )
        failed = sum(
            1 for outcome in test_outcomes if outcome.result == TestResult.FAIL
        )
        unable = sum(
            1
            for outcome in test_outcomes
            if outcome.result == TestResult.UNABLE_TO_EVALUATE
        )

        # Calculate overall score (exclude unable to evaluate from denominator)
        evaluable_outcomes = [
            o for o in test_outcomes if o.result != TestResult.UNABLE_TO_EVALUATE
        ]
        overall_score = (
            sum(o.score for o in evaluable_outcomes) / len(evaluable_outcomes)
            if evaluable_outcomes
            else 0.0
        )

        return EvaluationReport(
            document_title=document_title,
            language=language,
            total_tests=len(test_outcomes),
            passed_tests=passed,
            partial_tests=partial,
            failed_tests=failed,
            unable_to_evaluate=unable,
            overall_score=overall_score,
            test_outcomes=test_outcomes,
        )

    def print_report(self, report: EvaluationReport, detailed: bool = False) -> None:
        """Print evaluation report to console.

        Args:
            report: Evaluation report to display
            detailed: Whether to show detailed test results
        """
        self.console.print("\n" + "=" * 60)
        self.console.print("ðŸ§  [bold]Semantic Evaluation Report[/bold]")
        self.console.print("=" * 60)

        # Document info
        self.console.print(f"ðŸ“š Document: [bold]{report.document_title}[/bold]")
        self.console.print(f"ðŸŒ Language: {report.language}")
        self.console.print()

        # Summary table
        summary_table = Table(title="Test Summary")
        summary_table.add_column("Metric", style="bold cyan")
        summary_table.add_column("Count", justify="right", style="green")
        summary_table.add_column("Percentage", justify="right", style="yellow")

        summary_table.add_row(
            "âœ… Passed",
            str(report.passed_tests),
            f"{report.passed_tests / report.total_tests * 100:.1f}%",
        )
        summary_table.add_row(
            "ðŸŸ¡ Partial",
            str(report.partial_tests),
            f"{report.partial_tests / report.total_tests * 100:.1f}%",
        )
        summary_table.add_row(
            "âŒ Failed",
            str(report.failed_tests),
            f"{report.failed_tests / report.total_tests * 100:.1f}%",
        )
        summary_table.add_row(
            "â“ Unable",
            str(report.unable_to_evaluate),
            f"{report.unable_to_evaluate / report.total_tests * 100:.1f}%",
        )
        summary_table.add_row(
            "[bold]Total Tests[/bold]", f"[bold]{report.total_tests}[/bold]", "100.0%"
        )

        self.console.print(summary_table)
        self.console.print()

        # Overall score
        grade_color = self._get_grade_color(report.overall_score)
        self.console.print(
            f"ðŸŽ¯ Overall Score: [{grade_color}]{report.overall_score:.2f}[/{grade_color}] ({report.quality_grade})"
        )
        self.console.print(f"ðŸ“Š Pass Rate: {report.pass_rate:.1%}")
        self.console.print()

        # Detailed results
        if detailed:
            self.console.print("[bold]ðŸ“‹ Detailed Test Results:[/bold]")
            self.console.print()

            for outcome in report.test_outcomes:
                status_icon = self._get_status_icon(outcome.result)
                score_color = self._get_score_color(outcome.score)

                self.console.print(f"{status_icon} [bold]{outcome.test_name}[/bold]")
                self.console.print(
                    f"   Score: [{score_color}]{outcome.score:.2f}[/{score_color}]"
                )
                self.console.print(f"   Expected: {outcome.expected_answer}")
                self.console.print(f"   Found: {outcome.found_evidence}")

                if outcome.failure_reason:
                    self.console.print(f"   [red]Issue: {outcome.failure_reason}[/red]")

                self.console.print()

    def export_report(self, report: EvaluationReport, output_path: Path) -> None:
        """Export evaluation report to JSON file.

        Args:
            report: Evaluation report to export
            output_path: Path where to save the JSON report
        """
        report_data = {
            "document_title": report.document_title,
            "language": report.language,
            "evaluation_timestamp": None,  # Could add timestamp here
            "summary": {
                "total_tests": report.total_tests,
                "passed_tests": report.passed_tests,
                "partial_tests": report.partial_tests,
                "failed_tests": report.failed_tests,
                "unable_to_evaluate": report.unable_to_evaluate,
                "overall_score": report.overall_score,
                "pass_rate": report.pass_rate,
                "quality_grade": report.quality_grade,
            },
            "detailed_results": [
                {
                    "test_name": outcome.test_name,
                    "result": outcome.result.value,
                    "score": outcome.score,
                    "details": outcome.details,
                    "expected_answer": outcome.expected_answer,
                    "found_evidence": outcome.found_evidence,
                    "failure_reason": outcome.failure_reason,
                }
                for outcome in report.test_outcomes
            ],
        }

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)

        self.console.print(f"ðŸ“„ Report exported to: {output_path}")

    def _get_status_icon(self, result: TestResult) -> str:
        """Get emoji icon for test result."""
        icons = {
            TestResult.PASS: "âœ…",
            TestResult.PARTIAL: "ðŸŸ¡",
            TestResult.FAIL: "âŒ",
            TestResult.UNABLE_TO_EVALUATE: "â“",
        }
        return icons.get(result, "â“")

    def _get_grade_color(self, score: float) -> str:
        """Get color for overall grade."""
        if score >= 0.85:
            return "bright_green"
        elif score >= 0.75:
            return "green"
        elif score >= 0.65:
            return "yellow"
        elif score >= 0.50:
            return "orange"
        else:
            return "red"

    def _get_score_color(self, score: float) -> str:
        """Get color for individual test score."""
        if score >= 0.8:
            return "bright_green"
        elif score >= 0.6:
            return "green"
        elif score >= 0.4:
            return "yellow"
        else:
            return "red"


def main() -> int:
    """CLI entry point for semantic evaluation."""
    import argparse

    parser = argparse.ArgumentParser(
        description="Semantic Evaluation of Cognitive Reading Results"
    )
    parser.add_argument(
        "json_file", type=Path, help="Path to cognitive reader JSON output"
    )
    parser.add_argument(
        "--detailed", "-d", action="store_true", help="Show detailed test results"
    )
    parser.add_argument("--export", "-e", type=Path, help="Export report to JSON file")
    parser.add_argument(
        "--verbose", "-v", action="store_true", help="Verbose output during evaluation"
    )

    args = parser.parse_args()

    evaluator = SemanticEvaluator()

    try:
        report = evaluator.evaluate_document(args.json_file, verbose=args.verbose)
        evaluator.print_report(report, detailed=args.detailed)

        if args.export:
            evaluator.export_report(report, args.export)

    except Exception as e:
        print(f"Error during evaluation: {e}")
        return 1

    return 0


if __name__ == "__main__":
    exit(main())

# Cognitive Document Reader - Technical Specifications v3.0

> Complete technical specifications based on reverse engineering of the implemented project

---

## üéØ Executive Summary

**Cognitive Document Reader** is a Python library that implements cognitive document processing that simulates human reading patterns through a sequential multi-pass algorithm with cumulative context and source text authority principle.

### Primary Purpose

Generate high-quality hierarchical summaries and contextualized datasets for:
- **LLM Fine-tuning**: Datasets that preserve the voice and methodology of the original author
- **RAG Systems**: Chunks enriched with hierarchical context and key concepts
- **Human Reading**: Structured summaries that respect narrative flow

### Technical Differentiation

Unlike traditional document fragmentation systems, it implements:
- **Sequential algorithm** that processes sections in document order
- **Cumulative context** (parents + previous siblings) for each section
- **Authority principle** of source text over any contextual information
- **Multi-pass processing** with progressive contextual enrichment
- **Dual model** optimized for speed vs. quality depending on the pass

---

## üèóÔ∏è System Architecture

### Module Structure

```
src/cognitive_reader/
‚îú‚îÄ‚îÄ __init__.py                # Public API and exports
‚îú‚îÄ‚îÄ _version.py                # Automatic versioning
‚îú‚îÄ‚îÄ cli/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ main.py                # Complete command-line interface
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ progressive_reader.py  # Main cognitive reading engine
‚îÇ   ‚îî‚îÄ‚îÄ synthesizer.py         # Hierarchical synthesis and knowledge generation
‚îú‚îÄ‚îÄ llm/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ client.py              # Abstract LLM client with LangChain
‚îÇ   ‚îî‚îÄ‚îÄ prompts.py             # Centralized prompt management
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ config.py              # Configuration with Pydantic v2
‚îÇ   ‚îú‚îÄ‚îÄ document.py            # Document and section structures
‚îÇ   ‚îú‚îÄ‚îÄ knowledge.py           # Concept definitions and languages
‚îÇ   ‚îî‚îÄ‚îÄ llm_responses.py       # Structured response models
‚îú‚îÄ‚îÄ parsers/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ docling_parser.py      # Universal parser with docling
‚îÇ   ‚îî‚îÄ‚îÄ structure_detector.py  # Hierarchical structure detection
‚îî‚îÄ‚îÄ utils/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ language.py            # Language detection
    ‚îú‚îÄ‚îÄ structure_formatter.py # Structure formatting and filtering
    ‚îú‚îÄ‚îÄ text_cleaning.py       # Text cleaning
    ‚îî‚îÄ‚îÄ tokens.py              # Utilities for context and tokens
```

### Responsibility Pattern

#### **CognitiveReader** (Main Engine)
- Orchestrate complete cognitive reading flow
- Manage multi-pass processing
- Coordinate between components (parser, synthesizer, llm client)
- Apply development filters (max-depth, max-sections)

#### **DoclingParser** (Universal Parsing)
- Multi-format support: PDF, DOCX, HTML, Markdown
- Unified conversion to internal Markdown
- Hierarchical structure metadata extraction
- Configuration optimized for different document types

#### **StructureDetector** (Hierarchical Analysis)
- Automatic hierarchical structure detection
- Parent-child relationship construction
- Identification of sections with/without own content
- Document order index assignment

#### **LLMClient** (LLM Abstraction)
- Integration with LangChain for multiple providers
- Dual model management (fast/main)
- Structured responses with Pydantic
- Error handling and retries
- Development modes (dry-run, mock)

#### **Synthesizer** (Knowledge Generation)
- Bottom-up hierarchical synthesis
- Concept glossary generation
- Intelligent concept filtering
- Navigational metadata construction

#### **PromptManager** (Prompt Management)
- Versioned and consistent templates
- Multi-language support (English, Spanish)
- Authority principle enforcement in prompts
- Optimized for different cognitive passes

---

## ü§ñ Cognitive Reading Algorithm

### Sequential Processing with Cumulative Context

#### Core Algorithm Principles

1. **Document Order Processing**: Sections are processed in their natural order
2. **Cumulative Context**: Each section receives context from parents + previous siblings
3. **Source Text Authority**: Original text prevails over any contextual information
4. **Incremental Updates**: Parent levels are updated as children are processed
5. **Deferred Synthesis**: Parents without content wait for all children to be processed

#### Multi-Pass Architecture

**Pass 1:** Sequential algorithm with basic cumulative context
**Pass 2+:** Same sequential algorithm with enriched context (previous summaries + concept glossary)

### Authority Hierarchy

```
1. ü•á ORIGINAL SOURCE TEXT      ‚Üí Supreme authority, always prevails
2. ü•à CUMULATIVE CONTEXT       ‚Üí Parents and previous siblings summaries
3. ü•â PREVIOUS KNOWLEDGE       ‚Üí Summaries from previous passes
4. üìö CONCEPTUAL GLOSSARY      ‚Üí Supporting concept definitions
```

### Processing Flow Example

Using a complex document structure:

```
1. Process root section ‚Üí Generate root_summary_v1
2. Process first child ‚Üí Context: root_summary_v1
   ‚Üí Update root: root_summary_v1 + child1 ‚Üí root_summary_v2
3. Process second child ‚Üí Context: root_summary_v2 + child1_summary
   ‚Üí Update root: root_summary_v2 + child2 ‚Üí root_summary_v3
4. Continue sequentially maintaining cumulative context
5. Synthesize parents without content after all children processed
```

---

## üîß Technical Implementation

### Programming Language & Dependencies

#### Core Technology Stack
```toml
[project]
name = "cognitive-reader"
python = ">=3.10"
description = "Human-like document reading through progressive understanding"

[dependencies]
# Document Processing
docling = ">=2.40,<3.0"      # Universal document parser
langdetect = "^1.0.9"        # Language detection

# LLM Integration  
langchain = "^0.3.0"         # LLM abstraction layer
langchain-ollama = "^0.2.0"  # Ollama provider

# Data Models & Validation
pydantic = "^2.10.0"         # Configuration and response models

# CLI & HTTP
click = "^8.1.0"             # Command-line interface
aiohttp = "^3.10.0"          # Async HTTP operations

# Development Tools
ruff = "*"                   # Linting and formatting
mypy = "*"                   # Type checking
pytest = "*"                # Testing framework
pytest-asyncio = "*"        # Async testing support
```

### Configuration System

#### Environment-Driven Configuration
All configuration through environment variables with sensible defaults:

```python
class CognitiveConfig(BaseModel):
    """Complete configuration model with environment variable support."""
    
    # Core Models
    model_name: str = Field(default="qwen3:8b")
    fast_pass_model: str = Field(default="llama3.1:8b")
    main_model: str = Field(default="qwen3:8b")
    
    # Processing Parameters
    temperature: float = Field(default=0.1, ge=0.0, le=2.0)
    fast_pass_temperature: float = Field(default=0.05, ge=0.0, le=2.0)
    main_pass_temperature: float = Field(default=0.05, ge=0.0, le=2.0)
    
    # Context Management
    context_window: int = Field(default=4096, gt=1024)
    target_summary_words: int = Field(default=250, gt=20)
    
    # Multi-pass Control
    num_passes: int = Field(default=2, ge=1, le=10)
    enable_fast_first_pass: bool = Field(default=True)
    
    # Development Features
    dry_run: bool = Field(default=False)
    mock_responses: bool = Field(default=False)
    save_partial_results: bool = Field(default=False)
    show_context_usage: bool = Field(default=False)
```

#### Environment Variable Mapping
```bash
# Core Models
COGNITIVE_READER_MODEL=qwen3:8b
COGNITIVE_READER_FAST_PASS_MODEL=llama3.1:8b
COGNITIVE_READER_MAIN_MODEL=qwen3:8b

# Processing Parameters
COGNITIVE_READER_TEMPERATURE=0.1
COGNITIVE_READER_FAST_PASS_TEMPERATURE=0.05
COGNITIVE_READER_MAIN_PASS_TEMPERATURE=0.05

# Context Management
COGNITIVE_READER_CONTEXT_WINDOW=4096
COGNITIVE_READER_TARGET_SUMMARY_WORDS=250

# Multi-pass Control
COGNITIVE_READER_NUM_PASSES=2
COGNITIVE_READER_ENABLE_FAST_FIRST_PASS=true

# Development Features
COGNITIVE_READER_DRY_RUN=false
COGNITIVE_READER_MOCK_RESPONSES=false
COGNITIVE_READER_SAVE_PARTIAL_RESULTS=false
COGNITIVE_READER_SHOW_CONTEXT_USAGE=false
```

---

## üìÑ Document Processing Pipeline

### Processing Workflow

#### **Stage 1: Document Parsing**
```
Input: PDF, DOCX, HTML, Markdown
‚Üì
DoclingParser: Universal format conversion
‚Üì  
Output: Structured Markdown with metadata
```

#### **Stage 2: Structure Detection**
```
Input: Structured Markdown
‚Üì
StructureDetector: Hierarchical analysis
‚Üì
Output: DocumentSection tree with relationships
```

#### **Stage 3: Cognitive Reading**
```
Input: DocumentSection tree
‚Üì
CognitiveReader: Sequential multi-pass processing
‚Üì
Output: Section summaries + cumulative knowledge
```

#### **Stage 4: Knowledge Synthesis**
```
Input: Section summaries
‚Üì
Synthesizer: Hierarchical synthesis + concept extraction
‚Üì
Output: Complete CognitiveKnowledge structure
```

### Processing Strategy

#### **Strategy for Complex Documents**
- Documents >32k tokens, hierarchical structure: Extended context window
- Processing: Sequential with frequent incremental updates
- Context management: Intelligent truncation from beginning

#### **Format Fallback**
- **Model Fallback**: If fast_model fails, use main_model as backup
- **Format Fallback**: If a specific format fails, attempt plain text processing
- **Partial Processing**: Continue processing even with failing sections

#### **Development Features**
- **Depth filtering**: `--max-depth N` for development with large documents
- **Section limiting**: `--max-sections N` for quick testing
- **Partial results**: `--save-partials` for debugging intermediate states
- **Context monitoring**: `--show-context-usage` for optimization

---

## üöÄ Command Line Interface

### Basic Usage

#### Standard Processing
```bash
# Single document processing
cognitive-reader document.pdf

# With language specification
cognitive-reader document.pdf --language es

# With custom output directory
cognitive-reader document.pdf --output-dir ./results
```

#### Advanced Options
```bash
# Extended context for complex documents
cognitive-reader large-manual.pdf --extended-context

# Custom summary length (word-based)
cognitive-reader document.pdf --target-words 500 --max-words 800

# Development modes
cognitive-reader document.pdf --dry-run --show-context-usage
cognitive-reader document.pdf --max-depth 3 --save-partials
```

#### Multi-pass Configuration
```bash
# Single-pass processing (faster)
cognitive-reader document.pdf --single-pass

# Custom number of passes
cognitive-reader document.pdf --num-passes 3
```

### CLI Option Categories

#### **Document Processing**
- `document`: Input document path (required)
- `--language`: Document language (auto, en, es)
- `--output-dir`: Output directory for results
- `--output-format`: Output format (json, markdown, both)

#### **Processing Control**
- `--single-pass`: Disable multi-pass processing
- `--num-passes N`: Number of cognitive passes
- `--extended-context`: Use larger context window (8192 tokens)
- `--target-words N`: Target summary length in words
- `--min-words N` / `--max-words N`: Summary length bounds

#### **Development & Testing**
- `--dry-run`: No LLM calls, mock responses only
- `--max-depth N`: Limit processing depth
- `--max-sections N`: Limit number of sections
- `--save-partials`: Save intermediate results
- `--show-context-usage`: Display context window utilization

#### **Output Control**
- `--skip-glossary`: Disable concept glossary generation
- `--save-intermediate`: Save intermediate pass results
- `--validate-config-only`: Validate configuration and exit

---

## üìä Data Models & Structures

### Core Data Models

#### **DocumentSection**
```python
class DocumentSection(BaseModel):
    """Represents a hierarchical section of a document."""
    
    id: str                           # Unique section identifier
    title: str                        # Section title
    level: int                        # Hierarchical level (1=root)
    content: str                      # Section text content
    parent_id: Optional[str]          # Parent section ID
    children_ids: list[str]           # List of child section IDs
    order_index: int                  # Document order position
    metadata: dict[str, Any]          # Additional section metadata
```

#### **SectionSummary**
```python
class SectionSummary(BaseModel):
    """Summary of a processed document section."""
    
    section_id: str                   # Reference to original section
    title: str                        # Section title
    summary: str                      # Generated summary text
    key_concepts: list[str]           # Extracted key concepts
    parent_id: Optional[str]          # Hierarchical reference
    summary_length: int               # Summary length in characters
    processing_metadata: dict         # Processing context information
```

#### **ConceptDefinition**
```python
class ConceptDefinition(BaseModel):
    """Definition of a key concept from the document."""
    
    concept: str                      # Concept term
    definition: str                   # Generated definition
    first_mention_section: str        # Section where first mentioned
    related_sections: list[str]       # Sections where concept appears
    relevance_score: float            # Computed relevance (0.0-1.0)
    complexity_score: float           # Concept complexity indicator
```

#### **CognitiveKnowledge**
```python
class CognitiveKnowledge(BaseModel):
    """Complete knowledge structure from cognitive reading."""
    
    document_title: str               # Document title
    document_summary: str             # High-level document summary
    language: LanguageCode            # Detected/specified language
    
    # Hierarchical summaries
    section_summaries: dict[str, SectionSummary]
    
    # Knowledge artifacts
    concept_definitions: list[ConceptDefinition]
    
    # Processing metadata
    processing_stats: dict[str, Any]
    total_sections: int
    total_passes: int
    processing_time_seconds: float
```

### Response Models for LLM Integration

#### **Structured LLM Responses**
```python
class SectionSummaryResponse(BaseModel):
    """Structured response for section summarization."""
    
    summary: str = Field(description="Comprehensive section summary")
    key_concepts: list[str] = Field(description="Key concepts mentioned")

class ConceptDefinitionResponse(BaseModel):
    """Structured response for concept definition."""
    
    definition: str = Field(description="Clear concept definition")
    context: str = Field(description="Context where concept applies")
```

---

## üéõÔ∏è Configuration Management

### Development-Friendly Configuration

#### **Configuration Priorities**
1. **CLI arguments** (highest priority)
2. **Environment variables**
3. **Configuration file** (.env)
4. **Default values** (lowest priority)

#### **Key Configuration Categories**

**LLM Models & Parameters:**
```python
# Dual model strategy
fast_pass_model: str = "llama3.1:8b"    # Speed-optimized
main_model: str = "qwen3:8b"             # Quality-optimized

# Temperature control
fast_pass_temperature: float = 0.05      # Very conservative
main_pass_temperature: float = 0.05      # Very conservative
temperature: float = 0.1                 # Default fallback
```

**Context Management:**
```python
# Optimized context windows based on real usage data
context_window: int = 4096               # Default (2048 effective)
# Extended context via --extended-context = 8192 (4096 effective)

# Summary optimization (word-based for natural control)
target_summary_words: int = 250          # ~320 tokens
min_summary_words: int = 150             # ~190 tokens  
max_summary_words: int = 400             # ~500 tokens
```

**Multi-pass Control:**
```python
num_passes: int = 2                      # Standard dual-pass
enable_fast_first_pass: bool = True      # Fast/main model split
single_pass: bool = False                # Force single-pass override
```

**Development Features:**
```python
# AI agent development support
dry_run: bool = False                    # Mock responses only
mock_responses: bool = False             # Deterministic testing
save_partial_results: bool = False       # Debugging support
show_context_usage: bool = False         # Optimization insights

# Document filtering for development
max_hierarchy_depth: int = 10            # Section depth limit
max_sections: Optional[int] = None       # Section count limit
```

### Environment Variable Reference

#### **Main Variables**
```bash
# === CORE MODEL CONFIGURATION ===
COGNITIVE_READER_MODEL=qwen3:8b
COGNITIVE_READER_FAST_PASS_MODEL=llama3.1:8b
COGNITIVE_READER_MAIN_MODEL=qwen3:8b

# === TEMPERATURE CONTROL ===
COGNITIVE_READER_TEMPERATURE=0.1
COGNITIVE_READER_FAST_PASS_TEMPERATURE=0.05
COGNITIVE_READER_MAIN_PASS_TEMPERATURE=0.05

# === CONTEXT MANAGEMENT ===
COGNITIVE_READER_CONTEXT_WINDOW=4096
COGNITIVE_READER_TARGET_SUMMARY_WORDS=250
COGNITIVE_READER_MIN_SUMMARY_WORDS=150
COGNITIVE_READER_MAX_SUMMARY_WORDS=400

# === MULTI-PASS CONTROL ===
COGNITIVE_READER_NUM_PASSES=2
COGNITIVE_READER_ENABLE_FAST_FIRST_PASS=true
COGNITIVE_READER_SINGLE_PASS=false

# === DEVELOPMENT FEATURES ===
COGNITIVE_READER_DRY_RUN=false
COGNITIVE_READER_MOCK_RESPONSES=false
COGNITIVE_READER_SAVE_PARTIAL_RESULTS=false
COGNITIVE_READER_SHOW_CONTEXT_USAGE=false
COGNITIVE_READER_MAX_HIERARCHY_DEPTH=10

# === PERFORMANCE SETTINGS ===
COGNITIVE_READER_TIMEOUT_SECONDS=120
COGNITIVE_READER_MAX_RETRIES=3
COGNITIVE_READER_LLM_PROVIDER=ollama
COGNITIVE_READER_OLLAMA_BASE_URL=http://localhost:11434

# === LANGUAGE & FORMAT ===
COGNITIVE_READER_LANGUAGE=auto
COGNITIVE_READER_DOCUMENT_LANGUAGE=auto
```

---

## üß™ Testing & Quality Assurance

### Test Architecture

#### **Test Organization**
```
tests/
‚îú‚îÄ‚îÄ conftest.py              # Shared fixtures and configuration
‚îú‚îÄ‚îÄ integration/             # End-to-end integration tests
‚îÇ   ‚îî‚îÄ‚îÄ test_fast_first_pass.py
‚îî‚îÄ‚îÄ unit/                    # Unit tests by feature
    ‚îú‚îÄ‚îÄ test_cli.py          # CLI interface testing
    ‚îú‚îÄ‚îÄ test_cognitive_models.py    # Data model validation
    ‚îú‚îÄ‚îÄ test_language_detection.py  # Language detection tests
    ‚îú‚îÄ‚îÄ test_structure_formatter.py # Structure processing tests
    ‚îú‚îÄ‚îÄ test_text_cleaning.py       # Text utilities tests
    ‚îú‚îÄ‚îÄ test_unified_prompts.py     # Prompt management tests
    ‚îî‚îÄ‚îÄ test_multi_pass_features.py # Multi-pass processing tests
```

#### **Testing Strategy**

**Unit Tests:**
- All core components isolated
- Mock external dependencies (LLM calls, file I/O)
- Focus on business logic and edge cases
- Fast execution (< 1 second per test)

**Integration Tests:**
- End-to-end workflows with real components
- Use test documents of known structure
- Validate complete processing pipelines
- Mock only external LLM calls

**Test Coverage Goals:**
- Target: 90% code coverage minimum
- Critical paths: 100% coverage required
- Focus on error handling and edge cases

### Quality Standards

#### **Code Quality Requirements**

**Type Safety:**
- All functions must have complete type annotations
- Use strict mypy configuration
- Pydantic models for all data structures
- No `Any` types in production code

**Documentation:**
- All public functions must have Google-style docstrings
- Include purpose, parameters, returns, and examples
- Document complex algorithms with inline comments
- Maintain up-to-date README and specifications

**Error Handling:**
- Use specific exception types, never bare `except:`
- Create custom exceptions for domain-specific errors
- Provide informative error messages
- Log errors with appropriate context

#### **Development Workflow**

**Code Quality Checks:**
```bash
# Linting and formatting
uv run ruff check .
uv run ruff format .

# Type checking
uv run mypy src/

# Testing
uv run pytest
uv run pytest --cov=cognitive_reader --cov-report=html
```

**Git Workflow:**
- Feature-complete atomic commits
- Conventional commit messages
- Explicit file staging (never `git add .`)
- Each commit includes code + tests + docs

---

## üöÄ Deployment & Operations

### Production Deployment

#### **System Requirements**

**Minimum Requirements:**
- Python 3.10+
- 2GB RAM (for 4k context window)
- 1GB disk space
- Ollama server with compatible models

**Recommended Requirements:**
- Python 3.12+
- 4GB RAM (for extended context)
- 2GB disk space
- SSD storage for better I/O performance

#### **Installation Methods**

**Standard Installation:**
```bash
# From PyPI (when published)
pip install cognitive-reader

# From source
git clone https://github.com/juanje/cognitive-document-reader
cd cognitive-document-reader
uv sync
uv run cognitive-reader --help
```

**Docker Deployment:**
```dockerfile
FROM python:3.12-slim

# Install uv for fast dependency management
COPY --from=ghcr.io/astral-sh/uv:latest /uv /bin/uv

# Copy application
WORKDIR /app
COPY . .

# Install dependencies
RUN uv sync --frozen

# Set default command
ENTRYPOINT ["uv", "run", "cognitive-reader"]
```

### Operational Considerations

#### **Performance Monitoring**

**Key Metrics:**
- Processing time per document/section
- Context window utilization (`--show-context-usage`)
- Memory usage during processing
- LLM response times and error rates

**Resource Usage Optimization:**
```bash
# Monitor context utilization
cognitive-reader document.pdf --show-context-usage

# Process with resource limits
cognitive-reader document.pdf --max-sections 10 --max-depth 3

# Extended context only when needed
cognitive-reader complex-doc.pdf --extended-context
```

#### **Error Handling & Recovery**

**Retry Logic:**
- Automatic retry for transient LLM failures
- Configurable retry attempts and backoff
- Graceful degradation for partial processing

**Partial Processing Support:**
```bash
# Save intermediate results for debugging
cognitive-reader document.pdf --save-partials

# Continue processing even with section failures
# (built-in resilience)
```

#### **Scalability Considerations**

**Single Document Processing:**
- Designed for single-document, high-quality processing
- Sequential algorithm inherently limits parallelization
- Optimize through context window management

**Multiple Document Processing:**
- Run multiple instances in parallel
- Each instance processes one document
- Share Ollama server across instances

---

## üîÆ Future Extensibility

### Architecture for Evolution

#### **Extensible Components**

**LLM Provider Support:**
```python
# Current: Ollama-focused with LangChain abstraction
# Future: OpenAI, Anthropic, Google providers
class LLMClient:
    def _create_llm_provider(self) -> BaseChatModel:
        if self.config.llm_provider == "ollama":
            return ChatOllama(...)
        elif self.config.llm_provider == "openai":
            return ChatOpenAI(...)
        # Extensible for future providers
```

**Multi-language Support:**
```python
# Current: English, Spanish
# Future: Automatic prompt translation
class LanguageCode(str, Enum):
    EN = "en"
    ES = "es"
    # Future: FR, DE, IT, PT, etc.
```

**Output Format Extension:**
```python
# Current: JSON, Markdown
# Future: XML, YAML, custom formats
class OutputFormat(str, Enum):
    JSON = "json"
    MARKDOWN = "markdown"
    # Future: XML, YAML, etc.
```

#### **Algorithm Evolution**

**N-Pass Processing:**
```python
# Current: 2-pass optimized
# Future: Configurable N-pass processing
async def _n_pass_processing(self, sections, language, n_passes):
    for pass_num in range(1, n_passes + 1):
        if pass_num == 1:
            summaries = await self._sequential_processing(sections, language)
        else:
            summaries = await self._sequential_processing_with_enriched_context(
                sections, summaries, language
            )
    return summaries
```

**Advanced Context Strategies:**
- Semantic similarity for context selection
- Dynamic context window adjustment
- Cross-document context sharing

#### **Integration Possibilities**

**RAG System Integration:**
```python
# Future: Direct embedding generation
class CognitiveKnowledge(BaseModel):
    section_summaries: dict[str, SectionSummary]
    embeddings: Optional[dict[str, list[float]]]  # Future
    
    def to_vector_db(self) -> VectorCollection:
        """Export to vector database format."""
        pass
```

**Fine-tuning Dataset Export:**
```python
# Future: Training dataset format
def export_for_fine_tuning(self, format: str = "jsonl"):
    """Export summaries as fine-tuning dataset."""
    pass
```

### Roadmap Considerations

#### **Phase 2 Enhancements**
- Advanced context strategies (semantic selection)
- Multi-document processing with cross-references
- Integration with vector databases
- Web API for service deployment

#### **Phase 3 Possibilities**
- Real-time collaborative document processing
- Visual structure analysis for PDFs
- Domain-specific processing pipelines
- Integration with popular document management systems

---

## üìö Reference Implementation

### Complete Usage Example

#### **End-to-End Processing**
```python
from cognitive_reader import CognitiveReader
from cognitive_reader.models.config import CognitiveConfig

# Configure for high-quality processing
config = CognitiveConfig(
    # Dual model strategy
    enable_fast_first_pass=True,
    fast_pass_model="llama3.1:8b",
    main_model="qwen3:8b",
    
    # Multi-pass processing
    num_passes=2,
    
    # Context optimization
    context_window=4096,  # Standard documents
    target_summary_words=250,
    
    # Quality settings
    fast_pass_temperature=0.05,
    main_pass_temperature=0.05,
    
    # Language
    document_language="auto",
    
    # Development features
    show_context_usage=True,
    save_partial_results=False
)

# Process document
async def process_document():
    reader = CognitiveReader(config)
    knowledge = await reader.read_document_from_file("document.pdf")
    
    # Access results
    print(f"Document: {knowledge.document_title}")
    print(f"Sections processed: {knowledge.total_sections}")
    print(f"Processing time: {knowledge.processing_time_seconds:.1f}s")
    
    # Section summaries
    for section_id, summary in knowledge.section_summaries.items():
        print(f"\n{summary.title}:")
        print(f"  Summary: {summary.summary[:100]}...")
        print(f"  Concepts: {', '.join(summary.key_concepts[:3])}")
    
    # Concept glossary
    for concept in knowledge.concept_definitions[:5]:
        print(f"\n{concept.concept}: {concept.definition}")
    
    return knowledge
```

#### **CLI Equivalent**
```bash
# Same processing via CLI
cognitive-reader document.pdf \
    --num-passes 2 \
    --target-words 250 \
    --show-context-usage \
    --output-dir ./results \
    --output-format both
```

### Integration Examples

#### **RAG Pipeline Integration**
```python
from cognitive_reader import CognitiveReader
from cognitive_reader.models.config import CognitiveConfig

async def prepare_rag_chunks(document_path: str):
    """Process document for RAG system."""
    
    config = CognitiveConfig(
        target_summary_words=200,  # Optimal for embeddings
        context_window=4096,       # Standard processing
        skip_glossary=False        # Include concepts
    )
    
    reader = CognitiveReader(config)
    knowledge = await reader.read_document_from_file(document_path)
    
    # Extract RAG-ready chunks
    rag_chunks = []
    for summary in knowledge.section_summaries.values():
        chunk = {
            "id": summary.section_id,
            "title": summary.title,
            "content": summary.summary,
            "concepts": summary.key_concepts,
            "parent_id": summary.parent_id,
            "metadata": {
                "document": knowledge.document_title,
                "language": knowledge.language.value,
                "summary_length": summary.summary_length
            }
        }
        rag_chunks.append(chunk)
    
    # Include concept definitions as separate chunks
    for concept in knowledge.concept_definitions:
        chunk = {
            "id": f"concept_{concept.concept}",
            "title": f"Concept: {concept.concept}",
            "content": concept.definition,
            "concepts": [concept.concept],
            "metadata": {
                "type": "concept_definition",
                "relevance_score": concept.relevance_score,
                "related_sections": concept.related_sections
            }
        }
        rag_chunks.append(chunk)
    
    return rag_chunks
```

#### **Fine-tuning Dataset Export**
```python
async def create_fine_tuning_dataset(document_path: str):
    """Create fine-tuning dataset from processed document."""
    
    config = CognitiveConfig(
        num_passes=2,                    # High quality
        target_summary_words=300,        # Detailed summaries
        enable_fast_first_pass=True
    )
    
    reader = CognitiveReader(config)
    knowledge = await reader.read_document_from_file(document_path)
    
    training_examples = []
    
    # Create instruction-response pairs
    for summary in knowledge.section_summaries.values():
        # Find original section content
        original_section = None
        for section in reader.parsed_sections:
            if section.id == summary.section_id:
                original_section = section
                break
        
        if original_section and original_section.content:
            example = {
                "instruction": f"Summarize this section titled '{summary.title}' in approximately {config.target_summary_words} words, focusing on key concepts and main ideas:",
                "input": original_section.content,
                "output": summary.summary,
                "metadata": {
                    "document": knowledge.document_title,
                    "section_level": original_section.level,
                    "key_concepts": summary.key_concepts,
                    "language": knowledge.language.value
                }
            }
            training_examples.append(example)
    
    return training_examples
```

---

## üèÅ Conclusion

**Cognitive Document Reader v3.0** represents a mature, production-ready implementation of human-like document processing through:

### **Core Achievements**

1. **Authentic Cognitive Algorithm**: Sequential processing with cumulative context that mirrors human reading patterns

2. **Production Architecture**: Modular, type-safe, well-tested codebase with comprehensive error handling

3. **Flexible Configuration**: Environment-driven configuration with development-friendly features

4. **Multi-pass Processing**: Extensible N-pass architecture with dual model optimization

5. **Real-world Optimization**: Context window sizes based on empirical analysis of complex documents

### **Technical Excellence**

- **Type Safety**: Complete type annotations and Pydantic validation
- **Error Resilience**: Comprehensive error handling and recovery
- **Development Support**: Dry-run modes, partial results, context monitoring
- **Performance**: Optimized context windows and intelligent resource usage
- **Extensibility**: Clean architecture for future enhancements

### **Intended Applications**

- **RAG Systems**: High-quality chunks with hierarchical context
- **LLM Fine-tuning**: Training datasets that preserve author methodology
- **Document Analysis**: Automated processing of complex technical documents
- **Research Tools**: Academic and professional document comprehension

This specification serves as the definitive technical reference for implementing, maintaining, and extending the Cognitive Document Reader system. The architecture balances sophisticated cognitive algorithms with practical engineering considerations, making it suitable for both research applications and production deployments.

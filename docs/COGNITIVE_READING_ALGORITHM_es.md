# Algoritmo de Lectura Cognitiva

## Descripci√≥n General

El Lector Cognitivo de Documentos implementa un proceso de lectura que simula los patrones cognitivos humanos a trav√©s de procesamiento secuencial con contexto acumulativo. El sistema puede operar en modo de pasada √∫nica o m√∫ltiples pasadas, manteniendo siempre el principio de autoridad del texto fuente.

## Ejemplo de Estructura del Documento

Usando `examples/sample_document.md` como referencia:

> **üìù Nota**: El documento de ejemplo presenta "La teor√≠a de la conciencia cristalina de Aethelgard" - una **teor√≠a cient√≠fica completamente ficticia** creada espec√≠ficamente para probar las capacidades de lectura cognitiva. Contiene contenido sem√°ntico novedoso no presente en los datos de entrenamiento de LLM y redefine deliberadamente conceptos existentes para evaluar c√≥mo el sistema procesa informaci√≥n nueva vs. conocimiento pre-entrenado.

```
üìÑ La teor√≠a de la conciencia cristalina de Aethelgard (Padre con contenido)
‚îú‚îÄ‚îÄ üìñ Principios fundamentales (Padre con contenido)
‚îÇ   ‚îú‚îÄ‚îÄ üîπ La frecuencia resonante primordial (Hoja)
‚îÇ   ‚îî‚îÄ‚îÄ üîπ La refracci√≥n cognitiva (Hoja)
‚îú‚îÄ‚îÄ üìÅ El entramado som√°tico (Padre sin contenido)
‚îÇ   ‚îú‚îÄ‚îÄ üîπ La estructura del entramado (Hoja)
‚îÇ   ‚îî‚îÄ‚îÄ üîπ La resonancia emp√°tica: una redefinici√≥n (Hoja)
‚îî‚îÄ‚îÄ üìñ Aplicaciones y paradojas (Padre con contenido)
    ‚îú‚îÄ‚îÄ üîπ El concepto de la cristalizaci√≥n del yo (Hoja)
    ‚îî‚îÄ‚îÄ üîπ La paradoja del observador disonante (Hoja)
```

**Patrones Identificados:**
- **üìñ Padre con contenido**: Tiene texto introductorio propio + secciones hijas
- **üìÅ Padre sin contenido**: Solo organiza secciones hijas, requiere s√≠ntesis diferida
- **üîπ Secciones hoja**: Sin hijos, contienen contenido espec√≠fico

---

## Algoritmo Secuencial con Contexto Acumulativo

### Principios Fundamentales

El algoritmo procesa las secciones en **orden del documento** (secuencial top-down), manteniendo contexto acumulativo y aplicando el **principio de autoridad del texto fuente**.

```
PARA cada documento:
    1. DETECTAR estructura jer√°rquica
    2. ORDENAR secciones por secuencia del documento
    3. PROCESAR cada secci√≥n con contexto acumulativo
    4. ACTUALIZAR incrementalmente niveles superiores
    5. SINTETIZAR padres sin contenido al final
```

### Algoritmo Detallado

```
PARA cada secci√≥n en orden del documento:
    
    1. CONSTRUIR contexto acumulativo:
       - Res√∫menes de todos los padres
       - Res√∫menes de hermanos procesados previamente
    
    SI es padre CON contenido:
        2. PROCESAR texto propio con contexto ‚Üí generar resumen
        3. ACTUALIZAR incrementalmente niveles superiores
        
    SI es secci√≥n hoja:
        2. PROCESAR contenido con contexto acumulativo ‚Üí generar resumen
        3. ACTUALIZAR incrementalmente padre inmediato
        4. PROPAGAR actualizaciones hacia niveles superiores
        
    SI es padre SIN contenido:
        2. DIFERIR procesamiento hasta completar todos los hijos
        3. SINTETIZAR desde res√∫menes de hijos + contexto de padres
```

### Ejemplo Pr√°ctico: Procesamiento de `sample_document.md`

**1. Procesar "Cognitive Document Reader Example" (Nivel 1, Padre CON contenido)**
- Sin contexto previo (es la ra√≠z)
- Procesar su texto introductorio ‚Üí `resumen_ra√≠z_v1`

**2. Procesar "Principios fundamentales" (Nivel 2, Padre CON contenido)**
- Contexto: `resumen_ra√≠z_v1`
- Procesar su texto ‚Üí `resumen_principios_v1`
- Actualizar ra√≠z: `resumen_ra√≠z_v1 + resumen_intro_v1` ‚Üí `resumen_ra√≠z_v2`

**3. Procesar "Purpose" (Nivel 3, Hoja)**
- Contexto: `resumen_ra√≠z_v2 + resumen_intro_v1`
- Procesar contenido ‚Üí `resumen_purpose`
- Actualizar "Introduction": `resumen_intro_v1 + resumen_purpose` ‚Üí `resumen_intro_final`
- Actualizar ra√≠z: `resumen_ra√≠z_v2 + resumen_intro_final` ‚Üí `resumen_ra√≠z_v3`

**4. Procesar "Key Features" (Nivel 2, Padre SIN contenido)**
- Diferir hasta procesar todos sus hijos

**5. Procesar "1. Document Processing" (Nivel 3, Hoja)**
- Contexto: `resumen_ra√≠z_v3` (Key Features no tiene resumen a√∫n)
- Procesar contenido ‚Üí `resumen_doc_proc`

**6. Procesar "2. Language Detection" (Nivel 3, Hoja)**
- Contexto: `resumen_ra√≠z_v3 + resumen_doc_proc`
- Procesar contenido ‚Üí `resumen_lang_det`

**7. Procesar "3. Structured Output" (Nivel 3, Hoja)**
- Contexto: `resumen_ra√≠z_v3 + resumen_doc_proc + resumen_lang_det`
- Procesar contenido ‚Üí `resumen_struct_out`

**8. Sintetizar "Key Features" (S√≠ntesis Diferida)**
- Contexto de padres: `resumen_ra√≠z_v3`
- Sintetizar desde hijos: `resumen_doc_proc + resumen_lang_det + resumen_struct_out` ‚Üí `resumen_key_features`
- Actualizar ra√≠z: `resumen_ra√≠z_v3 + resumen_key_features` ‚Üí `resumen_ra√≠z_v4`

**9-11. Procesar "Technical Architecture" y sus hijos**
- Sigue el mismo patr√≥n que "Introduction"

**12. Procesar "Conclusion" (Nivel 2, Hoja)**
- Contexto: `resumen_ra√≠z_v5` (versi√≥n m√°s actualizada)
- Actualizaci√≥n final: `resumen_ra√≠z_v5 + resumen_conclusion` ‚Üí `resumen_ra√≠z_final`

---

## Procesamiento Multi-Pasada

### Arquitectura de M√∫ltiples Pasadas

El sistema puede ejecutar m√∫ltiples pasadas del mismo algoritmo secuencial, enriqueciendo progresivamente el contexto:

**Pasada 1:** Algoritmo secuencial con contexto b√°sico (padres + hermanos previos)
**Pasada 2+:** Mismo algoritmo secuencial con contexto enriquecido

### Contexto Enriquecido en Pasadas Posteriores

```
PARA cada secci√≥n en pasadas posteriores:
    
    1. CONSTRUIR contexto enriquecido:
       - Res√∫menes actuales de padres (pasada actual)
       - Resumen previo del mismo nodo (pasada anterior)
       - Glosario de conceptos clave con definiciones
    
    2. APLICAR principio de autoridad:
       - TEXTO FUENTE = autoridad suprema
       - CONTEXTO ENRIQUECIDO = informaci√≥n de apoyo
    
    3. PROCESAR con algoritmo secuencial id√©ntico
```

### Ejemplo: "Purpose" en Segunda Pasada

**Contexto Enriquecido:**
- **Padres actuales**: `resumen_ra√≠z_v2 + resumen_intro_v1` (segunda pasada)
- **Resumen previo**: `resumen_purpose_primera_pasada`
- **Conceptos**: `"progressive reading": "T√©cnica de procesamiento...", "hierarchical synthesis": "..."`

**Procesamiento:**
```
CONTEXTO (informaci√≥n de apoyo):
- Resumen Ra√≠z: [comprensi√≥n actual de la teor√≠a de Aethelgard]
- Resumen Principios fundamentales: [comprensi√≥n actual de la secci√≥n padre]
- Resumen Previo: [comprensi√≥n de primera pasada]
- Conceptos: [glosario con definiciones incluyendo frecuencia_resonante_primordial, refracci√≥n_cognitiva, etc.]

TEXTO FUENTE (autoridad suprema):
[contenido original de la secci√≥n "Purpose"]

El algoritmo genera un resumen refinado que integra:
- Fidelidad completa al texto fuente
- Enriquecimiento contextual de pasadas previas
- Marcos conceptuales del glosario
```

---

## Principio de Autoridad del Texto

### Jerarqu√≠a de Autoridad

```
1. ü•á TEXTO FUENTE ORIGINAL    ‚Üí Autoridad suprema, siempre prevalece
2. ü•à CONTEXTO ACUMULATIVO     ‚Üí Res√∫menes de padres y hermanos previos  
3. ü•â CONOCIMIENTO PREVIO      ‚Üí Res√∫menes de pasadas anteriores
4. üìö GLOSARIO CONCEPTUAL      ‚Üí Definiciones de apoyo
```

### Aplicaci√≥n del Principio

- **Conflicto texto vs contexto**: El texto fuente siempre gana
- **Prop√≥sito del contexto**: Enriquecer comprensi√≥n, no contradecir
- **Fidelidad garantizada**: Los res√∫menes reflejan fielmente el contenido original
- **Correcci√≥n autom√°tica**: Pasadas posteriores corrigen malentendidos previos bas√°ndose en el texto

---

## Caracter√≠sticas del Algoritmo

### Ventajas del Procesamiento Secuencial

- **Contexto Progresivo**: Cada secci√≥n se beneficia de toda la comprensi√≥n acumulada
- **Orden Natural**: Sigue la secuencia l√≥gica del documento como un lector humano
- **Actualizaciones Coherentes**: Los niveles superiores evolucionan incrementalmente
- **Integraci√≥n Sem√°ntica**: Los hermanos anteriores enriquecen el contexto de los siguientes

### Gesti√≥n de Complejidad

- **S√≠ntesis Diferida**: Los padres sin contenido esperan a que se procesen todos sus hijos
- **Contexto Truncado**: Se gestiona la longitud del contexto para mantenerse dentro de l√≠mites
- **Glosario Din√°mico**: Se generan conceptos clave que enriquecen pasadas posteriores
- **Autoridad Textual**: El principio de autoridad previene deriva sem√°ntica

---

## Resumen

El Algoritmo de Lectura Cognitiva simula la comprensi√≥n humana de documentos mediante:

1. **Procesamiento Secuencial**: Sigue el orden natural del documento
2. **Contexto Acumulativo**: Cada secci√≥n recibe contexto de padres y hermanos previos
3. **Actualizaciones Incrementales**: Los niveles superiores evolucionan conforme se procesan hijos
4. **Principio de Autoridad**: El texto fuente prevalece sobre cualquier contexto
5. **S√≠ntesis Adaptativa**: Estrategias espec√≠ficas para padres con/sin contenido
6. **Refinamiento Multi-Pasada**: Enriquecimiento progresivo manteniendo fidelidad textual

Este enfoque produce res√∫menes que mantienen la riqueza sem√°ntica del documento original, preservan la estructura jer√°rquica del autor, y proporcionan contexto adecuado tanto para lectura humana como para procesamiento de sistemas de IA.
---
description: LLM Integration rules for minimizing costs and ensuring robust error handling
globs: ["src/cognitive_reader/llm/**/*.py", "**/test*llm*.py"]
alwaysApply: false
---

# LLM Integration Rules

## 🤖 LLM Integration Philosophy

Focus on **efficiency, reliability, and cost optimization** while maintaining high-quality outputs.

## 🎯 Core Principles (CRITICAL)

### 1. Minimize LLM Calls
- **Every LLM call costs money and time**
- Batch operations whenever possible
- Implement intelligent caching strategies
- Reuse context between related operations
- Always provide dry-run modes for development

### 2. Robust Error Handling
- LLM services can fail unpredictably
- Implement exponential backoff retry logic
- Handle rate limits gracefully
- Provide meaningful fallback strategies

### 3. Context Window Management
- Track token usage carefully
- Implement smart chunking strategies
- Prioritize most relevant context
- Handle context overflow gracefully

## 🏗️ Implementation Patterns

### Async Client Pattern
```python
class LLMClient(ABC):
    """Abstract base class for LLM integrations."""
    
    @abstractmethod
    async def generate_summary(
        self,
        content: str,
        context: Optional[str] = None,
        prompt_type: str = "section_summary",
        **kwargs: Any
    ) -> str:
        """Generate summary for given content."""
        pass
    
    @abstractmethod
    async def validate_configuration(self) -> bool:
        """Validate LLM configuration without making expensive calls."""
        pass
```

### Retry Logic Pattern
```python
async def _generate_with_retry(
    self,
    prompt: str,
    max_retries: Optional[int] = None
) -> str:
    """Generate response with exponential backoff retry."""
    max_retries = max_retries or self.config.max_retries
    
    for attempt in range(max_retries + 1):
        try:
            return await self._call_llm_api(prompt)
        except aiohttp.ClientTimeout as e:
            if attempt == max_retries:
                raise LLMTimeoutError(f"LLM request timed out after {max_retries} retries") from e
            await asyncio.sleep(2 ** attempt)  # Exponential backoff
```

### Dry Run Pattern (MANDATORY)
```python
async def generate_summary(self, content: str, **kwargs) -> str:
    """Generate summary with dry-run support."""
    if self.config.dry_run:
        return self._generate_mock_summary(content, kwargs.get('prompt_type', 'summary'))
    
    return await self._generate_real_summary(content, **kwargs)

def _generate_mock_summary(self, content: str, prompt_type: str) -> str:
    """Generate mock summary for dry-run mode."""
    content_preview = content[:50].replace('\n', ' ')
    return f"[MOCK {prompt_type.upper()}] Summary of: {content_preview}..."
```

## 🚨 Required Exception Classes

```python
class LLMError(Exception):
    """Base exception for LLM operations."""
    pass

class LLMTimeoutError(LLMError):
    """Raised when LLM request times out."""
    pass

class LLMRateLimitError(LLMError):
    """Raised when rate limit is exceeded."""
    pass

class LLMConfigurationError(LLMError):
    """Raised when LLM configuration is invalid."""
    pass
```

## 📋 Mandatory Features

### Context Manager Support
```python
async def __aenter__(self):
    """Async context manager entry."""
    await self._setup_client()
    return self

async def __aexit__(self, exc_type, exc_val, exc_tb):
    """Async context manager exit."""
    if self.session:
        await self.session.close()
```

### Logging Requirements
```python
import logging

logger = logging.getLogger(__name__)

# Always log LLM operations for debugging and monitoring
async def generate_summary_with_logging(self, content: str, **kwargs) -> str:
    """Generate summary with comprehensive logging."""
    logger.debug(f"Generating summary for {len(content)} characters")
    
    try:
        start_time = time.time()
        result = await self.generate_summary(content, **kwargs)
        duration = time.time() - start_time
        
        logger.info(f"Summary generated successfully in {duration:.2f}s")
        logger.debug(f"Input tokens: ~{self.estimate_tokens(content)}")
        logger.debug(f"Output tokens: ~{self.estimate_tokens(result)}")
        
        return result
    except LLMError as e:
        logger.error(f"LLM error during summary generation: {e}")
        raise
```

## ⚡ Performance Requirements

### Controlled Concurrency
```python
async def generate_multiple_summaries(
    self,
    contents: List[str],
    max_concurrent: int = 3
) -> List[str]:
    """Generate multiple summaries with controlled concurrency."""
    semaphore = asyncio.Semaphore(max_concurrent)
    
    async def generate_with_semaphore(content: str) -> str:
        async with semaphore:
            return await self.generate_summary(content)
    
    tasks = [generate_with_semaphore(content) for content in contents]
    return await asyncio.gather(*tasks)
```

**Remember: LLM operations are expensive and can fail. Always implement robust error handling, provide dry-run modes for development, and optimize for minimal API calls while maintaining high-quality outputs.**